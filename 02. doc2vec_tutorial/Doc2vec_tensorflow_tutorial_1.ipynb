{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **[Tensorflow] Doc2vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from konlpy.tag import Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./input/ratings_train.txt', delimiter='\\t')\n",
    "pos_df = train_df[train_df['label'] == 1]['document']\n",
    "neg_df = train_df[train_df['label'] == 0]['document']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Data preprocessing**\n",
    "#### **(1) Text를 word의 seq로 변경**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = []\n",
    "pos_label = []\n",
    "for pos in pos_df:\n",
    "    try:\n",
    "        words = komoran.nouns(pos)\n",
    "        # 단어가 3개 이상 등장한 review 에 대해서만 데이터 set을 생성한다.\n",
    "        if len(words) > 3:\n",
    "            pos_data.append(words)\n",
    "            # 긍정을 의미하는 label 이다.\n",
    "            pos_label.append(1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data = []\n",
    "neg_label = []\n",
    "for neg in neg_df:\n",
    "    try:\n",
    "        words = komoran.nouns(neg)\n",
    "        if len(words) > 3:\n",
    "            neg_data.append(words)\n",
    "            neg_label.append(0)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pos_data + neg_data\n",
    "labels = pos_label + neg_label\n",
    "\n",
    "print(len(texts))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) Word의 seq를 number의 seq로 변경 (word2vec의 input : 정수)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing dictionary\n",
    "words = []\n",
    "for text in texts:\n",
    "    words.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "voca_size = 25000\n",
    "corpus = {}\n",
    "\n",
    "# 빈도수가 높은 단어 순서대로, indexing이 된다.\n",
    "for word, freq in Counter(words).most_common(voca_size):\n",
    "    corpus[word] = len(corpus)\n",
    "\n",
    "# 결과 디버깅을 위하여, number seq에서, number에 대응하는 word를 찾기 위한 lookup dictionary\n",
    "corpus_rev = dict(zip(corpus.values(), corpus.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number seq를 생성한다. 즉 단어에 대응되는 num의 seq로 바꾸는 작업\n",
    "num_seqs = []\n",
    "\n",
    "for text in texts:\n",
    "    num_seq = []\n",
    "    for word in text:\n",
    "        if word in corpus:\n",
    "            idx = corpus[word]\n",
    "        else:\n",
    "            idx = 0  # 없는 단어는 0으로 한다. voca_size를 지정했기 때문이다.\n",
    "        num_seq.append(idx)\n",
    "    num_seqs.append(num_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) Input batch (source context words + doc id, target word)의 tuple를 생성**\n",
    " - Doc2vec은 document id를 source context words에 추가한다.\n",
    " - 학습 모델은 source context words + document id -> target word가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seqs : [src_ctx_word1, src_ctx_word2, target], doc_id\n",
    "# Inputs : [[src_ctx_word1, src_ctx_word2, doc_id], [target]]\n",
    "# Batch : [ src_ctx_word1 (+) src_ctx_word2 , doc_id ], label : [ target ]\n",
    "def generate_batch_data(num_seqs, batch_size, window_size):\n",
    "    # Batch 데이터\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    while len(batch_data) < batch_size:\n",
    "        # Batch 데이터를 생성하고자 하는 문서의 문서 번호(문서 내에서의 순서 index)\n",
    "        random_seq_idx = int(np.random.choice(len(num_seqs), size=1))\n",
    "        random_seq = num_seqs[random_seq_idx]\n",
    "        \n",
    "        sources_and_target = []\n",
    "        for idx in range(window_size, len(random_seq)):\n",
    "            sources = random_seq[idx-window_size : idx]\n",
    "            target = random_seq[idx]\n",
    "            sources_and_target.append((sources, target))\n",
    "        # Target word, source context word를 분리한다.\n",
    "        sources_list, target_list = [list(x) for x in zip(*sources_and_target)]\n",
    "        # 문서 번호 추가\n",
    "        sources_list = [srcs + [random_seq_idx] for srcs in sources_list]\n",
    "        # 데이터를 생성한다.\n",
    "        batch_data.extend(sources_list)\n",
    "        label_data.extend(target_list)\n",
    "                \n",
    "    # 마지막 iteration에서 batch size를 초과할 수 있으므로, batch 크기만 생성\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    # Placeholder에 feeding하기 위해서는 numpy array 형태로 변환한다.\n",
    "    batch_data = np.array(batch_data)\n",
    "    # Nce_loss에서 사용하기 위해서 shape을 변경한다.\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set을 생성한다. 생성한 embedding matrix와의 거리를 측정할 수 있다.\n",
    "# 이것은 단순히 similarity를 보여주기 위한 예이다.\n",
    "valid_words = ['한국', '사랑', '스릴러', '장르']\n",
    "valid_words = [corpus[elem] for elem in valid_words]\n",
    "valid_datasets = tf.constant(valid_words, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Modeling using tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "doc_embedding_size = 128\n",
    "concat_size = embedding_size + doc_embedding_size\n",
    "batch_size = 128\n",
    "window_size = 3  # How many subnets to consider left and right\n",
    "num_sampled = 64  # Number of negative examples to sample\n",
    "learning_rate = 0.001\n",
    "num_steps = 50000  # Tensorflow tutorial\n",
    "num_docs = len(num_seqs)\n",
    "\n",
    "# Input data\n",
    "with tf.name_scope('inputs'):\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size, window_size + 1])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "\"\"\"\n",
    "Embedding -> Train input -> embed\n",
    "[[1,2,3], -> [0,2]       -> [[1,2,3],\n",
    " [4,5,6],                    [7,8,9]]\n",
    " [7,8,9]]\n",
    "\"\"\"\n",
    "with tf.device('/GPU:0'):\n",
    "    with tf.name_scope('embeddings'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([voca_size, embedding_size], -1, 1))\n",
    "        doc_embeddings = tf.Variable(tf.random_uniform([num_docs, doc_embedding_size], -1.0, 1.0))\n",
    "        # Source context word의 embedding 합\n",
    "        embed = tf.zeros([batch_size, embedding_size])\n",
    "        for idx in range(window_size):\n",
    "            embed += tf.nn.embedding_lookup(embeddings, train_inputs[:, idx])\n",
    "        # Doc indices\n",
    "        doc_indices = tf.slice(train_inputs, [0, window_size], [batch_size, 1])\n",
    "        # Doc embedding\n",
    "        doc_embed = tf.nn.embedding_lookup(doc_embeddings, doc_indices)\n",
    "        # Source context word의 embedding을 더한 값과, doc의 embedding을 concat\n",
    "        final_embed = tf.concat(axis=1, values=[embed, tf.squeeze(doc_embed)])\n",
    "\n",
    "    # Weights\n",
    "    with tf.name_scope('weights'):  \n",
    "        nce_weights = tf.Variable(tf.truncated_normal([voca_size, concat_size],\n",
    "                                              stddev = 1.0 / np.sqrt(concat_size)))\n",
    "    # Biases\n",
    "    with tf.name_scope('biases'):\n",
    "        nce_biases = tf.Variable(tf.zeros([voca_size]))\n",
    "\n",
    "# Loss\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=final_embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=voca_size))\n",
    "\n",
    "# Optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 단어 집합과 임베딩된 모든 단어 사이의 코사인 유사도를 구해보자.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "# 검증 단어 집합의 embedding을 구한다.\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_datasets)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session 초기화\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch_data(num_seqs, batch_size, window_size)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "    if i % 100 == 0:\n",
    "        print(\"=================================\")\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        # Githup에 올릴때는 주석을 한다. 결과를 보고 싶으면 주석을 해제하면 된다.\n",
    "        print('Loss at step {}: {}'.format(i, loss_val))\n",
    "        \n",
    "        # Valid word와의 similarity를 확인하여 검증을 해보자.\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = corpus_rev[valid_words[j]]\n",
    "            top_k = 5\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = 'Nearest to {}:'.format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = corpus_rev[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            # Githup에 올릴때 주석, 결과를 보고 싶으면 주석을 해제하면 된다.\n",
    "            print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
