{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **[Tensorflow] Doc2vec과 LSTM을 이용한 분류기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from konlpy.tag import Komoran\n",
    "import gensim\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./input/ratings_train.txt', delimiter='\\t')\n",
    "pos_df = train_df[train_df['label'] == 1]['document']\n",
    "neg_df = train_df[train_df['label'] == 0]['document']\n",
    "\n",
    "test_df = pd.read_csv('./input/ratings_test.txt', delimiter='\\t')\n",
    "test_pos_df = test_df[test_df['label'] == 1]['document']\n",
    "test_neg_df = test_df[test_df['label'] == 0]['document']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data preprocessing**\n",
    "### **(1) Text를 word의 seq로 변경**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78986\n",
      "78986\n"
     ]
    }
   ],
   "source": [
    "pos_data = []\n",
    "pos_label = []\n",
    "for pos in pos_df:\n",
    "    try:\n",
    "        words = komoran.nouns(pos)\n",
    "        # 단어가 3개 이상 등장한 review 에 대해서만 데이터 set을 생성한다.\n",
    "        if len(words) > 3:\n",
    "            pos_data.append(words)\n",
    "            # 긍정을 의미하는 label 이다.\n",
    "            pos_label.append(1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "neg_data = []\n",
    "neg_label = []\n",
    "for neg in neg_df:\n",
    "    try:\n",
    "        words = komoran.nouns(neg)\n",
    "        if len(words) > 3:\n",
    "            neg_data.append(words)\n",
    "            neg_label.append(0)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "texts = pos_data + neg_data\n",
    "labels = pos_label + neg_label\n",
    "\n",
    "print(len(texts))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['포스터', '초딩', '영화', '줄', '오버', '연기'],\n",
       " ['익살', '연기', '영화', '스파이더맨', '커스틴 던스트'],\n",
       " ['액션', '재미', '안', '영화'],\n",
       " ['평점', '것', '만', '헐리우드', '식'],\n",
       " ['볼', '때', '눈물', '년대', '향수', '자극', '허진호', '감성', '절제', '멜로', '달인']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26457\n",
      "26457\n"
     ]
    }
   ],
   "source": [
    "test_pos_data = []\n",
    "test_pos_label = []\n",
    "for pos in test_pos_df:\n",
    "    try:\n",
    "        words = komoran.nouns(pos)\n",
    "        # 단어가 3개 이상 등장한 review 에 대해서만 데이터 set을 생성한다.\n",
    "        if len(words) > 3:\n",
    "            test_pos_data.append(words)\n",
    "            # 긍정을 의미하는 label 이다.\n",
    "            test_pos_label.append(1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "test_neg_data = []\n",
    "test_neg_label = []\n",
    "for neg in test_neg_df:\n",
    "    try:\n",
    "        words = komoran.nouns(neg)\n",
    "        if len(words) > 3:\n",
    "            test_neg_data.append(words)\n",
    "            test_neg_label.append(0)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "test_texts = test_pos_data + test_neg_data\n",
    "test_labels = test_pos_label + test_neg_label\n",
    "\n",
    "print(len(test_texts))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['음악', '주가', '최고', '음악', '영화'],\n",
       " ['이별', '아픔', '뒤', '인연', '기쁨', '사람'],\n",
       " ['청춘', '이성', '찰나', '포착', '수채화', '퀴어', '영화'],\n",
       " ['눈', '반전', '영화', '흡인력'],\n",
       " ['13일의 금요일',\n",
       "  '나이트메어',\n",
       "  '시리즈',\n",
       "  '시리즈',\n",
       "  '양산',\n",
       "  '레이저',\n",
       "  '시리즈',\n",
       "  '편',\n",
       "  '작가',\n",
       "  '상상력',\n",
       "  '작품',\n",
       "  '갈고리',\n",
       "  '고어',\n",
       "  '씨',\n",
       "  '충격']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2) Word의 seq를 number의 seq로 변경**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing dictionary\n",
    "words = []\n",
    "for text in texts:\n",
    "    words.extend(text)\n",
    "    \n",
    "voca_size = 25000\n",
    "corpus = {}\n",
    "\n",
    "# 빈도수가 높은 단어 순서대로, indexing이 된다.\n",
    "for word, freq in Counter(words).most_common(voca_size):\n",
    "    corpus[word] = len(corpus)\n",
    "\n",
    "# 결과 디버깅을 위하여, number seq에서, number에 대응하는 word를 찾기 위한 lookup dictionary\n",
    "corpus_rev = dict(zip(corpus.values(), corpus.keys()))\n",
    "\n",
    "# Number seq를 생성한다. 즉 단어에 대응되는 num의 seq로 바꾸는 작업\n",
    "num_seqs = []\n",
    "\n",
    "for text in texts:\n",
    "    num_seq = []\n",
    "    for word in text:\n",
    "        if word in corpus:\n",
    "            idx = corpus[word]\n",
    "        else:\n",
    "            idx = 0  # 없는 단어는 0으로 한다. voca_size를 지정했기 때문이다.\n",
    "        num_seq.append(idx)\n",
    "    num_seqs.append(num_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing dictionary\n",
    "test_words = []\n",
    "for test_text in test_texts:\n",
    "    test_words.extend(test_text)\n",
    "    \n",
    "voca_size = 25000\n",
    "test_corpus = {}\n",
    "\n",
    "# 빈도수가 높은 단어 순서대로, indexing이 된다.\n",
    "for test_word, freq in Counter(test_words).most_common(voca_size):\n",
    "    corpus[test_word] = len(test_corpus)\n",
    "\n",
    "# 결과 디버깅을 위하여, number seq에서, number에 대응하는 word를 찾기 위한 lookup dictionary\n",
    "test_corpus_rev = dict(zip(test_corpus.values(), test_corpus.keys()))\n",
    "\n",
    "# Number seq를 생성한다. 즉 단어에 대응되는 num의 seq로 바꾸는 작업\n",
    "test_num_seqs = []\n",
    "\n",
    "for test_text in test_texts:\n",
    "    test_num_seq = []\n",
    "    for test_word in test_text:\n",
    "        if test_word in test_corpus:\n",
    "            idx = test_corpus[test_word]\n",
    "        else:\n",
    "            idx = 0  # 없는 단어는 0으로 한다. voca_size를 지정했기 때문이다.\n",
    "        test_num_seq.append(idx)\n",
    "    test_num_seqs.append(test_num_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_text = texts + test_texts\n",
    "merged_labels = labels + test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Word embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1) Doc2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc와 doc_id를 가지는 TaggedDocument의 generator를 생성한다.\n",
    "class LabledLineSentence(object):\n",
    "    def __init__(self, doc_list, doc_id_list):\n",
    "        self.doc_list = doc_list\n",
    "        self.doc_id_list = doc_id_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield gensim.models.doc2vec.TaggedDocument(words=doc, tags=[self.doc_id_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = LabledLineSentence(merged_text, merged_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model = gensim.models.Doc2Vec(\n",
    "    vector_size=128, window=3, alpha=0.025, min_alpha=0.025, min_count=2, workers=multiprocessing.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19202"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.build_vocab(it)\n",
    "doc2vec_model.train(it, total_examples=len(merged_text), epochs=100)\n",
    "\n",
    "len(doc2vec_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2) Word2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.Word2Vec(\n",
    "    merged_text, size=128, window=1, min_count=2, workers=multiprocessing.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. LSTM for classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1) Input, output 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We change LabelEncoder's return shape.\n",
    "class ReshapedLabelEncoder(LabelEncoder):\n",
    "    def fit_transform(self, y, *args, **kwargs):\n",
    "        return super().fit_transform(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = ReshapedLabelEncoder()\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('label_encoder', label_encoder),\n",
    "    ('onehot_encoder', onehot_encoder)\n",
    "])\n",
    "\n",
    "# For output\n",
    "onehot_labels = pipeline.fit_transform(labels).toarray()\n",
    "test_onehot_labels = pipeline.fit_transform(test_labels).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2) LSTM 모델 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter 설정\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "SEQUENCE_LENGTH = 10\n",
    "HIDDEN_SIZE = 128\n",
    "DIM_INPUT = 128\n",
    "DIM_OUTPUT = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "X = tf.placeholder(tf.float32, shape=[None, SEQUENCE_LENGTH, DIM_INPUT])\n",
    "t = tf.placeholder(tf.float32, shape=[None, DIM_OUTPUT])\n",
    "batch_size = tf.placeholder(tf.int32, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "def inference(x, batch_size):\n",
    "    # Cell를 정의\n",
    "    lstm_cell = tf.nn.rnn_cell.MultiRNNCell([\n",
    "        tf.nn.rnn_cell.LSTMCell(num_units=HIDDEN_SIZE),\n",
    "        tf.nn.rnn_cell.LSTMCell(num_units=HIDDEN_SIZE),\n",
    "        tf.nn.rnn_cell.LSTMCell(num_units=HIDDEN_SIZE),\n",
    "    ])\n",
    "    # TODO : Dropout 설정\n",
    "    # Initial state 정의\n",
    "    initial_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    # Cell output 정의\n",
    "    cell_outputs, state = tf.nn.dynamic_rnn(lstm_cell, x, initial_state=initial_state, dtype=tf.float32)\n",
    "    final_cell_output = cell_outputs[:, -1, :]\n",
    "    # Weight matrix\n",
    "    V = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, DIM_OUTPUT]))\n",
    "    # Biases\n",
    "    c = tf.Variable(tf.zeros([DIM_OUTPUT]))\n",
    "    return tf.matmul(final_cell_output, V) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss_func(y, t):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y, labels=t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "def train(loss):\n",
    "    return tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect graph nodes\n",
    "y = inference(X, batch_size)\n",
    "loss = loss_func(y, t)\n",
    "train_step = train(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3) Batch data 생성**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "targets = []\n",
    "\n",
    "for text, onehot_label in zip(texts, onehot_labels):\n",
    "    if len(text) < SEQUENCE_LENGTH:\n",
    "        temp_input_data = []\n",
    "        num_na = SEQUENCE_LENGTH - len(text)\n",
    "        text = text + num_na * ['NA']\n",
    "        for word in text:\n",
    "            if word == 'NA':\n",
    "                temp_input_data.append(np.zeros([DIM_INPUT]))\n",
    "            else:\n",
    "                try:\n",
    "                    # Word2vec vs Doc2vec\n",
    "                    temp_input_data.append(doc2vec_model.wv.get_vector(word))\n",
    "                except:\n",
    "                    temp_input_data.append(np.zeros([DIM_INPUT]))\n",
    "        input_data.append(np.array(temp_input_data))\n",
    "        targets.append(onehot_label)\n",
    "    else:\n",
    "        for i in range(len(text) - SEQUENCE_LENGTH):\n",
    "            temp_input_data = []\n",
    "            for word in text[i:i+SEQUENCE_LENGTH]:\n",
    "                try:\n",
    "                    temp_input_data.append(doc2vec_model.wv.get_vector(word))\n",
    "                except:\n",
    "                    temp_input_data.append(np.zeros([DIM_INPUT]))\n",
    "            input_data.append(np.array(temp_input_data))\n",
    "            targets.append(onehot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN, LSTM cell의 input 형태로 reshape한다.\n",
    "input_data = np.array(input_data).reshape(-1, SEQUENCE_LENGTH, DIM_INPUT)\n",
    "targets = np.array(targets).reshape(-1, DIM_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154867, 10, 128)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data = []\n",
    "test_targets = []\n",
    "\n",
    "for text, onehot_label in zip(test_texts, test_onehot_labels):\n",
    "    if len(text) < SEQUENCE_LENGTH:\n",
    "        temp_input_data = []\n",
    "        num_na = SEQUENCE_LENGTH - len(text)\n",
    "        text = text + num_na * ['NA']\n",
    "        for word in text:\n",
    "            if word == 'NA':\n",
    "                temp_input_data.append(np.zeros([DIM_INPUT]))\n",
    "            else:\n",
    "                try:\n",
    "                    # Word2vec vs Doc2vec\n",
    "                    temp_input_data.append(doc2vec_model.wv.get_vector(word))\n",
    "                except:\n",
    "                    temp_input_data.append(np.zeros([DIM_INPUT]))\n",
    "        test_input_data.append(np.array(temp_input_data))\n",
    "        test_targets.append(onehot_label)\n",
    "    else:\n",
    "        for i in range(len(text) - SEQUENCE_LENGTH):\n",
    "            temp_input_data = []\n",
    "            for word in text[i:i+SEQUENCE_LENGTH]:\n",
    "                try:\n",
    "                    temp_input_data.append(doc2vec_model.wv.get_vector(word))\n",
    "                except:\n",
    "                    temp_input_data.append(np.zeros([DIM_INPUT]))\n",
    "            test_input_data.append(np.array(temp_input_data))\n",
    "            test_targets.append(onehot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN, LSTM cell의 input 형태로 reshape한다.\n",
    "test_input_data = np.array(test_input_data).reshape(-1, SEQUENCE_LENGTH, DIM_INPUT)\n",
    "test_targets = np.array(test_targets).reshape(-1, DIM_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52262, 10, 128)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4) 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch's final - train loss: 0.431219, validation_acc: 0.795635, test_acc: 0.738816\n",
      "1 epoch's final - train loss: 0.345797, validation_acc: 0.844127, test_acc: 0.745972\n",
      "2 epoch's final - train loss: 0.191326, validation_acc: 0.865952, test_acc: 0.740155\n",
      "3 epoch's final - train loss: 0.270534, validation_acc: 0.873830, test_acc: 0.741457\n",
      "4 epoch's final - train loss: 0.171740, validation_acc: 0.874411, test_acc: 0.732789\n",
      "5 epoch's final - train loss: 0.187230, validation_acc: 0.866662, test_acc: 0.726321\n",
      "6 epoch's final - train loss: 0.086780, validation_acc: 0.881255, test_acc: 0.735085\n",
      "7 epoch's final - train loss: 0.122334, validation_acc: 0.882611, test_acc: 0.736596\n",
      "8 epoch's final - train loss: 0.093612, validation_acc: 0.880610, test_acc: 0.731277\n",
      "9 epoch's final - train loss: 0.115922, validation_acc: 0.879512, test_acc: 0.729536\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_data, targets, test_size = 0.1)\n",
    "num_batches = len(X_train) // BATCH_SIZE\n",
    "num_validation = len(X_test)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    X_samp, y_samp = shuffle(X_train, y_train)\n",
    "    for i in range(num_batches):\n",
    "        start = i * BATCH_SIZE\n",
    "        end = start + BATCH_SIZE\n",
    "        \n",
    "        _, train_loss = sess.run([train_step, loss], feed_dict={\n",
    "            X: X_samp[start:end],\n",
    "            t: y_samp[start:end],\n",
    "            batch_size: BATCH_SIZE\n",
    "        })\n",
    "        \n",
    "    validation_acc = sess.run(accuracy, feed_dict={\n",
    "        X: X_test,\n",
    "        t: y_test,\n",
    "        batch_size: num_validation\n",
    "    })\n",
    "\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        X: test_input_data,\n",
    "        t: test_targets,\n",
    "        batch_size: len(test_input_data)\n",
    "    })\n",
    "        \n",
    "    print(\"%d epoch's final - train loss: %f, validation_acc: %f, test_acc: %f\"\n",
    "          %(epoch, train_loss, validation_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7295358"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.eval(session=sess, feed_dict={\n",
    "    X: test_input_data,\n",
    "    t: test_targets,\n",
    "    batch_size: len(test_input_data)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
