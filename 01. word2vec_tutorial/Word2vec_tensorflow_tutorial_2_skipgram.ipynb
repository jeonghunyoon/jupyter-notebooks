{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **[Tensorflow] SKIP_GRAM을 이용한 word2vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from konlpy.tag import Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./input/ratings_train.txt', delimiter='\\t')\n",
    "pos_df = train_df[train_df['label'] == 1]['document']\n",
    "neg_df = train_df[train_df['label'] == 0]['document']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Data preprocessing**\n",
    "#### **(1) Text를 word의 seq로 변경**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = []\n",
    "pos_label = []\n",
    "for pos in pos_df:\n",
    "    try:\n",
    "        words = komoran.nouns(pos)\n",
    "        # 단어가 3개 이상 등장한 review 에 대해서만 데이터 set을 생성한다.\n",
    "        if len(words) > 3:\n",
    "            pos_data.append(words)\n",
    "            # 긍정을 의미하는 label 이다.\n",
    "            pos_label.append(1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data = []\n",
    "neg_label = []\n",
    "for neg in neg_df:\n",
    "    try:\n",
    "        words = komoran.nouns(neg)\n",
    "        if len(words) > 3:\n",
    "            neg_data.append(words)\n",
    "            neg_label.append(0)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pos_data + neg_data\n",
    "labels = pos_label + neg_label\n",
    "\n",
    "print(len(texts))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) Word의 seq를 number의 seq로 변경 (word2vec의 input : 정수)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing dictionary\n",
    "words = []\n",
    "for text in texts:\n",
    "    words.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "voca_size = 25000\n",
    "corpus = {}\n",
    "\n",
    "# 빈도수가 높은 단어 순서대로, indexing이 된다.\n",
    "for word, freq in Counter(words).most_common(voca_size):\n",
    "    corpus[word] = len(corpus)\n",
    "\n",
    "# 결과 디버깅을 위하여, number seq에서, number에 대응하는 word를 찾기 위한 lookup dictionary\n",
    "corpus_rev = dict(zip(corpus.values(), corpus.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number seq를 생성한다. 즉 단어에 대응되는 num의 seq로 바꾸는 작업\n",
    "num_seqs = []\n",
    "\n",
    "for text in texts:\n",
    "    num_seq = []\n",
    "    for word in text:\n",
    "        if word in corpus:\n",
    "            idx = corpus[word]\n",
    "        else:\n",
    "            idx = 0  # 없는 단어는 0으로 한다. voca_size를 지정했기 때문이다.\n",
    "        num_seq.append(idx)\n",
    "    num_seqs.append(num_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) Input batch (target word, source context word)의 tuple를 생성**\n",
    " - SKIP-GRAM는 target word를 이용하여 source context word를 예측하는 학습과정을 가진다.\n",
    " - Source context word left, target word, source context right 이렇게 주어지면, 학습 모델은 target word -> source context left, target word -> source context right가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seqs : [ src_ctx_left, target, src_ctx_right ]\n",
    "# Inputs : [ (target, src_ctx_left), (target, src_ctx_right) ]\n",
    "# Batch : [ target ], label : [ src_ctx_left ]\n",
    "#         [ target ],         [ src_ctx_right ]\n",
    "def generate_batch_data(num_seqs, batch_size, window_size):\n",
    "    # Batch 데이터\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    while len(batch_data) < batch_size:\n",
    "        # Random하게 review data 1개를 뽑는다.\n",
    "        random_seq = np.random.choice(num_seqs)\n",
    "        span = skip_window * 2 + 1\n",
    "        \n",
    "        windows_seq = []\n",
    "        for i in range(len(random_seq) - span + 1):\n",
    "            windows_seq.append(random_seq[i:i+span])\n",
    "            \n",
    "        # Target index는 가운데에 있는 것이 될 것이다.\n",
    "        target_indices = [span//2 for _ in range(len(windows_seq))]\n",
    "        \n",
    "        # Target word, source context word list의 tuple을 생성\n",
    "        target_and_sources = [(seq[idx], seq[:idx] + seq[idx+1:]) \n",
    "                              for idx, seq in zip(target_indices, windows_seq)]\n",
    "\n",
    "        # Target word, source context word의 tuple을 생성\n",
    "        target_and_source = [(target, source) \n",
    "                             for target, sources in target_and_sources \n",
    "                             for source in sources]\n",
    "        \n",
    "        # Target word list(input), source context word list(output)로 나눈다.\n",
    "        target_word_list, src_ctx_word_list = [list(elem) for elem in zip(*target_and_source)]\n",
    "        batch_data.extend(target_word_list)\n",
    "        label_data.extend(src_ctx_word_list)\n",
    "                \n",
    "    # 마지막 iteration에서 batch size를 초과할 수 있으므로, batch 크기만 생성\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    # Placeholder에 feeding하기 위해서는 numpy array 형태로 변환한다.\n",
    "    batch_data = np.array(batch_data)\n",
    "    # Nce_loss에서 사용하기 위해서 shape을 변경한다.\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set을 생성한다. 생성한 embedding matrix와의 거리를 측정할 수 있다.\n",
    "# 이것은 단순히 similarity를 보여주기 위한 예이다.\n",
    "valid_words = ['한국', '사랑', '스릴러', '장르']\n",
    "valid_words = [corpus[elem] for elem in valid_words]\n",
    "valid_datasets = tf.constant(valid_words, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. modeling using tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "batch_size = 128\n",
    "skip_window = 1  # How many subnets to consider left and right\n",
    "num_sampled = 64  # Number of negative examples to sample\n",
    "learning_rate = 0.001\n",
    "num_steps = 1000001  # Tensorflow tutorial\n",
    "\n",
    "# Input data\n",
    "with tf.name_scope('inputs'):\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "\"\"\"\n",
    "Embedding -> Train input -> embed\n",
    "[[1,2,3], -> [0,2]       -> [[1,2,3],\n",
    " [4,5,6],                    [7,8,9]]\n",
    " [7,8,9]]\n",
    "\"\"\"\n",
    "with tf.device('/GPU:0'):\n",
    "    with tf.name_scope('embeddings'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([voca_size, embedding_size], -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Weights\n",
    "    with tf.name_scope('weights'):\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([voca_size, embedding_size], \n",
    "                                stddev = 1.0 / math.sqrt(embedding_size)))\n",
    "    # Biases\n",
    "    with tf.name_scope('biases'):\n",
    "        nce_biases = tf.Variable(tf.zeros([voca_size]))\n",
    "\n",
    "# Loss\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=nce_weights,\n",
    "            biases=nce_biases,\n",
    "            labels=train_labels,\n",
    "            inputs=embed,\n",
    "            num_sampled=num_sampled,\n",
    "            num_classes=voca_size))\n",
    "\n",
    "# Optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 단어 집합과 임베딩된 모든 단어 사이의 코사인 유사도를 구해보자.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "# 검증 단어 집합의 embedding을 구한다.\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_datasets)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session 초기화\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 50000\n",
    "for i in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch_data(num_seqs, batch_size, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "    if i % 100 == 0:\n",
    "        print(\"=================================\")\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        # Githup에 올릴때는 주석을 한다. 결과를 보고 싶으면 주석을 해제하면 된다.\n",
    "        print('Loss at step {}: {}'.format(i, loss_val))\n",
    "        \n",
    "        # Valid word와의 similarity를 확인하여 검증을 해보자.\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = corpus_rev[valid_words[j]]\n",
    "            top_k = 5\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = 'Nearest to {}:'.format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = corpus_rev[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            # Githup에 올릴때 주석, 결과를 보고 싶으면 주석을 해제하면 된다.\n",
    "            print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
