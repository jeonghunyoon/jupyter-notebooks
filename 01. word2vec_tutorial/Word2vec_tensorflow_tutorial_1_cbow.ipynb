{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **[Tensorflow] CBOW을 이용한 word2vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from konlpy.tag import Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./input/ratings_train.txt', delimiter='\\t')\n",
    "pos_df = train_df[train_df['label'] == 1]['document']\n",
    "neg_df = train_df[train_df['label'] == 0]['document']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Data preprocessing**\n",
    "#### **(1) Text를 word의 seq로 변경**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = []\n",
    "pos_label = []\n",
    "for pos in pos_df:\n",
    "    try:\n",
    "        words = komoran.nouns(pos)\n",
    "        # 단어가 3개 이상 등장한 review 에 대해서만 데이터 set을 생성한다.\n",
    "        if len(words) > 3:\n",
    "            pos_data.append(words)\n",
    "            # 긍정을 의미하는 label 이다.\n",
    "            pos_label.append(1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data = []\n",
    "neg_label = []\n",
    "for neg in neg_df:\n",
    "    try:\n",
    "        words = komoran.nouns(neg)\n",
    "        if len(words) > 3:\n",
    "            neg_data.append(words)\n",
    "            neg_label.append(0)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pos_data + neg_data\n",
    "labels = pos_label + neg_label\n",
    "\n",
    "print(len(texts))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) Word의 seq를 number의 seq로 변경 (word2vec의 input : 정수)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing dictionary\n",
    "words = []\n",
    "for text in texts:\n",
    "    words.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "voca_size = 25000\n",
    "corpus = {}\n",
    "\n",
    "# 빈도수가 높은 단어 순서대로, indexing이 된다.\n",
    "for word, freq in Counter(words).most_common(voca_size):\n",
    "    corpus[word] = len(corpus)\n",
    "\n",
    "# 결과 디버깅을 위하여, number seq에서, number에 대응하는 word를 찾기 위한 lookup dictionary\n",
    "corpus_rev = dict(zip(corpus.values(), corpus.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number seq를 생성한다. 즉 단어에 대응되는 num의 seq로 바꾸는 작업\n",
    "num_seqs = []\n",
    "\n",
    "for text in texts:\n",
    "    num_seq = []\n",
    "    for word in text:\n",
    "        if word in corpus:\n",
    "            idx = corpus[word]\n",
    "        else:\n",
    "            idx = 0  # 없는 단어는 0으로 한다. voca_size를 지정했기 때문이다.\n",
    "        num_seq.append(idx)\n",
    "    num_seqs.append(num_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) Input batch (target word, source context word)의 tuple를 생성**\n",
    " - CBOW는 source context word를 이용하여 target word를 예측하는 학습과정을 가진다.\n",
    " - Source context word left, target word, source context right 이렇게 주어지면, 학습 모델은 source context left -> target word, source context right -> target word가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seqs : [ src_ctx_left, target, src_ctx_right ]\n",
    "# Inputs : [ [src_ctx_left, src_ctx_right], [target] ]\n",
    "# Batch : [ src_ctx_left (+) src_ctx_right ], label : [ target ] \n",
    "\n",
    "# Window size는 src_ctx_left ~ target까지의 크기이다.\n",
    "def generate_batch_data(num_seqs, batch_size, window_size):\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    while len(batch_data) < batch_size:\n",
    "        # 시작점을 random하게 선택한다.\n",
    "        random_seq = np.random.choice(num_seqs)\n",
    "        sources_and_target = []\n",
    "        for idx in range(window_size, len(random_seq) - window_size):\n",
    "            target = random_seq[idx]\n",
    "            sources = random_seq[idx-window_size : idx] + random_seq[idx+1 : idx+window_size+1]\n",
    "            sources_and_target.append((sources, target))\n",
    "            \n",
    "        # Target word, source context word를 분리한다.\n",
    "        sources_list, target_list = [list(x) for x in zip(*sources_and_target)]\n",
    "        # 데이터를 생성한다.\n",
    "        batch_data.extend(sources_list)\n",
    "        label_data.extend(target_list)\n",
    "    \n",
    "    # batch 크기만 생성\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    # Placeholder에 feeding하기 위해서는 numpy array 형태로!\n",
    "    batch_data = np.array(batch_data)\n",
    "    # Nce_loss에서 사용하기 위해서 shape을 변경한다.\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set을 생성한다. 생성한 embedding matrix와의 거리를 측정할 수 있다.\n",
    "# 이것은 단순히 similarity를 보여주기 위한 예이다.\n",
    "valid_words = ['한국', '사랑', '스릴러', '장르']\n",
    "valid_words = [corpus[elem] for elem in valid_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Modeling using tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "cbow_window = 1  # How many words to consider left and right\n",
    "num_sampled = 64  # Number of negative examples to sample\n",
    "learning_rate = 0.001\n",
    "num_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding 초기화\n",
    "embeddings = tf.Variable(tf.random_uniform([voca_size, embedding_size], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source context word\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size, 2*cbow_window])\n",
    "# Target word\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# 모델의 validation을 체크해본다.\n",
    "valid_dataset = tf.constant(valid_words, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW는 source context word의 embedding을 모두 더한 값을 사용한다.\n",
    "embed = tf.zeros([batch_size, embedding_size])\n",
    "for idx in range(2*cbow_window):\n",
    "    embed += tf.nn.embedding_lookup(embeddings, train_inputs[:, idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nce_weights = tf.Variable(tf.truncated_normal([voca_size, embedding_size],\n",
    "                                              stddev = 1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([voca_size]))\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=train_labels,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=voca_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding 동작 과정을 확인하기 위하여 코사인 유사도를 이용\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_words)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 정의\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for i in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch_data(num_seqs, batch_size, cbow_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "    \n",
    "    # loss\n",
    "    if i % 100 == 0:\n",
    "        print(\"=================================\")\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        # Githup에 올릴때 주석, 결과를 보고 싶으면 주석을 해제하면 된다.\n",
    "        print('Loss at step {}: {}'.format(i, loss_val))\n",
    "        \n",
    "        # Validation word와의 similarity를 구한다.\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for idx, num in enumerate(valid_words):\n",
    "            valid_word = corpus_rev[num]\n",
    "            top_k = 5\n",
    "            nearest = (-sim[idx, :]).argsort()[1:top_k+1]\n",
    "            log_str = 'Nearest to {}:'.format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = corpus_rev[nearest[k]]\n",
    "                log_str = '{} {},'.format(log_str, close_word)\n",
    "            # Githup에 올릴때 주석, 결과를 보고 싶으면 주석을 해제하면 된다.\n",
    "            print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
