{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author : Jeonghun Yoon\n",
    "\n",
    "이번 노트북의 주제는 **Autoencoder** 입니다. **Autoencoder**는 input data와 output data가 비슷한(거의 동일한) 신경망 구조입니다. 이 신경망 구조의 핵심 아이디어는 input data에 내제되어 있는 특성 즉 *latent representation*을 잘 찾아내는 것입니다. 다양한 autoencoder 응용 모델 중에서, machine translation에 사용되는, Sequence-to-Sequence 모델인 **RNN Encoder-Decoder**에 대해서 공부하도록 하겠습니다. \n",
    "\n",
    "Reference는 아래의 논문들입니다.\n",
    " 1. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    " 2. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    " \n",
    "Sequence-to-Sequnece 모델인 **RNN Encoder-Decoder**의 input을 $(x_1,...,x_{T})$, output을 $(y_1,...,y_{T'})$ 라고 하겠습니다. 목표는 $p(y_1,...,y_{T'}|x_1,...,x_T)$ 인 조건부 확률모델을 학습하는 것입니다 [1],[2].\n",
    "RNN Encoder-Decoder 모델은 2개의 phase로 구성됩니다.\n",
    " - Encoder : input sequence $(x_1,...,x_{T})$를 순서대로 읽는 RNN 모델입니다. RNN 모델이기 때문에 hidden state를 가지고 있고, hidden state의 update 조건은 $\\textbf{h}_t=f(\\textbf{h}_{t-1}, x_t)$ 입니다. $f$는 non-linear activation 함수 입니다.\n",
    " - Decoder : (output sequence에서) 다음에 나올 symbol인 $y_t$를 예측(prediction)하면서 output seqeunce를 생성하는 학습모델입니다. 기본적인 RNN과 달리 이전 time stamp에서의 출력이 RNN의 입력으로 사용됩니다. hidden state의 update 조건은 $\\textbf{h}_t=f(\\textbf{h}_{t-1}, y_{t-1}, \\textbf{c})$ 이며 $\\textbf{c}$에는 input sequence 전체의 정보(fixed dimensional representation)가 축약되어 있습니다. 따라서 output sequence에서, 다음 symbol에 대한 조건부 확률은 $P(y_t|y_{t-1},...,y_1,\\textbf{c})=g(\\textbf{h}_t, y_{t-1}, \\textbf{c})$ 입니다. $g$는 유효한 확률값을 생성해야 하므로 softmax function과 같은 함수가 될 것입니다.\n",
    " \n",
    "따라서 우리가 구하고자 하는 조건부 확률모델은 아래와 같습니다.\n",
    "$$p(y_1,...,y_{T'}|x_1,...,x_T)=\\prod_{t=1}^{T'}p(y_t|y_{t-1},...,y_1,\\textbf{c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tensorflow version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정수 계산기를 RNN Encoder-Decoder를 이용하여 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import typing\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### const.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 300\n",
    "INPUT_SEQUENCE_LENGTH = 7\n",
    "EMBEDDING_DIM = 12\n",
    "OUTPUT_SEQUENCE_LENGTH = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data_helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Data preprocessing\n",
    "=====================\n",
    "\n",
    "Tutorial에서 사용할 데이터 sample 조건은 다음과 같습니다. \n",
    " - 덧셈 연산\n",
    " - 정수의 최대 자리수(digits)는 3자리\n",
    "\n",
    "ex) \n",
    "123+456=579, 87+138=225, ... \n",
    " - '123+456', '87+138'은 input sequence\n",
    " - '579', '225'는 output sequence\n",
    "\"\"\"  \n",
    "\n",
    "def _get_num(digits):\n",
    "    num = ''\n",
    "    for _ in range(np.random.randint(1, digits+1)):\n",
    "        num += np.random.choice(list('0123456789'))\n",
    "    return int(num)\n",
    "\n",
    "def _padding(seq, max_len):\n",
    "    \"\"\"Fix the input sequence length for padding the input sequence.\"\"\"\n",
    "    return seq + ' ' *(max_len - len(seq))\n",
    "\n",
    "def _get_input_n_output_seq(digits):\n",
    "    \"\"\"Input sequence와 output sequence pair를 생성합니다.\"\"\"\n",
    "    num1 = _get_num(digits)\n",
    "    num2 = _get_num(digits)\n",
    "    question = '{}+{}'.format(num1, num2)\n",
    "    answer = str(num1 + num2)\n",
    "    return list(_padding(question, 2 * digits + 1)), list(_padding(answer, digits + 1))\n",
    "\n",
    "def create_num_seqs(digits=3, data_size=50000):\n",
    "    input_seqs = []\n",
    "    output_seqs = []\n",
    "\n",
    "    for _ in list(range(data_size)):\n",
    "        input_seq, output_seq = _get_input_n_output_seq(digits)\n",
    "        input_seqs.append(input_seq)\n",
    "        output_seqs.append(output_seq)\n",
    "\n",
    "    return input_seqs, output_seqs\n",
    "\n",
    "def create_onehot_seqs(input_num_seqs, output_num_seqs):\n",
    "    \"\"\"Create one hot vectors\"\"\"\n",
    "    label_bin = LabelBinarizer()\n",
    "    label_bin.fit(list(' +0123456789'))\n",
    "\n",
    "    input_onehot_seqs = [label_bin.transform(seq) for seq in input_num_seqs]\n",
    "    output_onehot_seqs = [label_bin.transform(seq) for seq in output_num_seqs]\n",
    "    \n",
    "    return input_onehot_seqs, output_onehot_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voca table\n",
    "voca_lookup = {}\n",
    "for idx, ch in enumerate(list(' +0123456789')):\n",
    "    voca_lookup[idx] = ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num sequences\n",
    "input_num_seqs, output_num_seqs = create_num_seqs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot sequences\n",
    "input_onehot_seqs, output_onehot_seqs = create_onehot_seqs(input_num_seqs, output_num_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM(encoder) input shape\n",
    "input_onehot_seqs = np.array(input_onehot_seqs, dtype=np.float32).reshape([-1, INPUT_SEQUENCE_LENGTH, EMBEDDING_DIM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### hparams.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 128\n",
    "forget_bias = 1.0\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Encoder and decoder\n",
    "======================\n",
    "\n",
    "Incoder와 decoder를 정의합니다.\n",
    "Layer가 1인 encoder입니다.\n",
    "\"\"\"\n",
    "\n",
    "def inference(X, t, batch_size, is_training):\n",
    "    # 1. Encoder\n",
    "    encoder = tf.nn.rnn_cell.LSTMCell(num_hidden, forget_bias=forget_bias)\n",
    "    initial_state = encoder.zero_state(batch_size, tf.float32)\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder, \n",
    "                                                        X, \n",
    "                                                        initial_state=initial_state, \n",
    "                                                        dtype=tf.float32)\n",
    "    final_encoder_output = encoder_outputs[:, -1, :]\n",
    "\n",
    "    # 2. Decoder : 학습의 경우와 학습이 아닌 경우를 나누어야 합니다.\n",
    "    with tf.variable_scope('Decoder'):\n",
    "        decoder = tf.nn.rnn_cell.LSTMCell(num_hidden, forget_bias=forget_bias)\n",
    "        # Variables for logits\n",
    "        V = tf.Variable(tf.truncated_normal([num_hidden, EMBEDDING_DIM], stddev=0.01, dtype=tf.float32))\n",
    "        c = tf.Variable(tf.zeros([EMBEDDING_DIM], dtype=tf.float32))\n",
    "        if is_training is True:\n",
    "            \"\"\"학습인 경우는, target sequence가 input으로 주어집니다. 따라서 input을 그대로 사용하면 됩니다.\"\"\"\n",
    "            decoder_outputs, decoder_states = tf.nn.dynamic_rnn(decoder,\n",
    "                                                                t, \n",
    "                                                                initial_state=encoder_state,\n",
    "                                                                dtype=tf.float32)\n",
    "            # First element of output : Encoder final output\n",
    "            # Previous output is the input of current state!\n",
    "            outputs = tf.concat([tf.reshape(final_encoder_output, [-1, 1, num_hidden]), decoder_outputs[:,0:OUTPUT_SEQUENCE_LENGTH-1,:]], axis=1)\n",
    "            # Logits\n",
    "            logits = tf.einsum('ijk,kl->ijl', outputs, V) + c\n",
    "            return logits\n",
    "        else:\n",
    "            \"\"\"학습이 아닌경우는, 직전의 output을 target sequence를 예측하기 위한 input으로 다시 사용합니다.\"\"\"\n",
    "            state = encoder_state\n",
    "            decoder_outputs = [final_encoder_output]\n",
    "            outputs = []\n",
    "            for i in range(1, OUTPUT_SEQUENCE_LENGTH):\n",
    "                if i > 1:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                logit_val = tf.matmul(decoder_outputs[-1], V) + c\n",
    "                prob_val = tf.nn.softmax(logit_val)\n",
    "                # 모델의 출력값\n",
    "                outputs.append(prob_val)\n",
    "                # 이전의 모델 출력값이, 입력으로 사용됩니다. 따라서, 입력값과 동일하게 onehot vector로 만들어줍니다.\n",
    "                prob_one_hot = tf.one_hot(tf.argmax(prob_val, -1), depth=OUTPUT_SEQUENCE_LENGTH)\n",
    "                # 현재의 모델 출력값\n",
    "                output, state = decoder(prob_one_hot, state)\n",
    "                decoder_outputs.append(output)\n",
    "            # 모델의 마지막 출력값을 구한다.\n",
    "            final_logit = tf.matmul(decoder_outputs[-1], V) + c\n",
    "            final_prob = tf.nn.softmax(final_logit)\n",
    "            outputs.append(final_prob)\n",
    "            outputs = tf.reshape(tf.concat(outputs, axis=1), [-1, OUTPUT_SEQUENCE_LENGTH, EMBEDDING_DIM])\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Learning functions\n",
    "=====================\n",
    "\n",
    "학습에 사용되는 function들을 정의합니다.\n",
    "\"\"\"\n",
    "\n",
    "def loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "\n",
    "def train_op(loss):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    return optimizer.minimize(loss)\n",
    "\n",
    "def accuracy(y, t):\n",
    "    y = tf.nn.softmax(y)\n",
    "    correct_pred = tf.equal(tf.argmax(y, -1), tf.argmax(t, -1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_X = input_onehot_seqs[:5]\n",
    "# test_y = output_onehot_seqs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.run(logits, feed_dict={X: test_X, y: test_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. Placeholder 정의\n",
    "=====================\n",
    "\n",
    "학습에 사용되는 Placeholder들을 정의합니다.\n",
    "\"\"\"\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, INPUT_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "t = tf.placeholder(dtype=tf.float32, shape=(None, OUTPUT_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "batch_size = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "is_training = tf.placeholder(dtype=tf.bool, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. Train step 정의\n",
    "=====================\n",
    "\n",
    "Inference, loss, optimizer를 정의합니다.\n",
    "\"\"\"\n",
    "y = inference(X, t, batch_size, is_training)\n",
    "loss_val = loss(y, t)\n",
    "train_step = train_op(loss_val)\n",
    "acc_val = accuracy(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(input_onehot_seqs, output_onehot_seqs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### executor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    samp_X, samp_y = shuffle(train_X, train_y)\n",
    "    num_batch = int(len(train_X) / BATCH_SIZE)\n",
    "    for i in range(num_batch):\n",
    "        start = i * BATCH_SIZE\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={\n",
    "            X: samp_X[start:end],\n",
    "            t: samp_y[start:end],\n",
    "            batch_size: BATCH_SIZE,\n",
    "            is_training: True\n",
    "        })\n",
    "    val_loss = loss_val.eval(session=sess, feed_dict={\n",
    "        X: test_X,\n",
    "        t: test_y,\n",
    "        batch_size: len(test_X),\n",
    "        is_training: False\n",
    "    })\n",
    "    acc_loss = acc_val.eval(session=sess, feed_dict={\n",
    "        X: test_X,\n",
    "        t: test_y,\n",
    "        batch_size: len(test_X),\n",
    "        is_training: False\n",
    "    })\n",
    "    print('epoch: {}, loss: {}, accuracy: {}'.format(epoch, val_loss, acc_loss))\n",
    "    # 검증 데이터\n",
    "    if epoch > 150:\n",
    "        for i in range(3):\n",
    "            idx = np.random.randint(0, len(test_X))\n",
    "            question = test_X[idx].reshape([-1, INPUT_SEQUENCE_LENGTH, EMBEDDING_DIM])\n",
    "            answer = test_y[idx].reshape([-1, OUTPUT_SEQUENCE_LENGTH, EMBEDDING_DIM])\n",
    "            prediction = y.eval(session=sess, feed_dict={\n",
    "                X: question,\n",
    "                t: answer,\n",
    "                batch_size: 1,\n",
    "                is_training: True\n",
    "            })\n",
    "            question = question.argmax(axis=-1)\n",
    "            answer = answer.argmax(axis=-1)\n",
    "            prediction = np.argmax(prediction, axis=-1)\n",
    "\n",
    "            question = ''.join(voca_lookup[i] for i in question[0])\n",
    "            answer = ''.join(voca_lookup[i] for i in answer[0])\n",
    "            prediction = ''.join(voca_lookup[i] for i in prediction[0])\n",
    "\n",
    "            print('==========')\n",
    "            print('Q: {}, A: {}, P: {}'.format(question, answer, prediction))\n",
    "            if answer == prediction: \n",
    "                print('True')\n",
    "            else:\n",
    "                print('False')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
